import { AIModel, AIModelResponse, getOptimalModel, classifyPrompt, AI_MODELS } from './ai-models';

export interface AIRequest {
  prompt: string;
  context?: string;
  projectId?: string;
  userId?: string;
  userTier: 'free' | 'pro' | 'enterprise';
  preferredModel?: string;
  streaming?: boolean;
}

export interface AIJobLog {
  id: string;
  timestamp: string;
  prompt: string;
  modelUsed: string;
  promptType: string;
  response: string;
  duration: number;
  tokensUsed: number;
  cost: number;
  success: boolean;
  error?: string;
}

export class AIOrchestrator {
  private jobLogs: AIJobLog[] = [];
  private activeJobs: Map<string, AbortController> = new Map();

  async processRequest(request: AIRequest): Promise<{
    response: AIModelResponse;
    jobLog: AIJobLog;
  }> {
    const startTime = Date.now();
    const jobId = `job_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    
    try {
      // Classify the prompt to determine optimal model
      const promptType = classifyPrompt(request.prompt);
      
      // Get optimal model based on prompt type and user tier
      const selectedModel = request.preferredModel 
        ? AI_MODELS.find(m => m.id === request.preferredModel) || getOptimalModel(promptType, request.userTier)
        : getOptimalModel(promptType, request.userTier);

      console.log(`ü§ñ AI Orchestrator: Using ${selectedModel.name} for ${promptType} task`);

      // Create abort controller for this job
      const abortController = new AbortController();
      this.activeJobs.set(jobId, abortController);

      // Process the request with the selected model
      const response = await this.callModel(selectedModel, request, abortController.signal);
      
      const duration = Date.now() - startTime;
      const cost = response.usage.totalTokens * selectedModel.costPerToken;

      // Create job log
      const jobLog: AIJobLog = {
        id: jobId,
        timestamp: new Date().toISOString(),
        prompt: request.prompt,
        modelUsed: selectedModel.id,
        promptType,
        response: response.content,
        duration,
        tokensUsed: response.usage.totalTokens,
        cost,
        success: true
      };

      this.jobLogs.push(jobLog);
      this.activeJobs.delete(jobId);

      return { response, jobLog };

    } catch (error) {
      const duration = Date.now() - startTime;
      const jobLog: AIJobLog = {
        id: jobId,
        timestamp: new Date().toISOString(),
        prompt: request.prompt,
        modelUsed: request.preferredModel || 'unknown',
        promptType: classifyPrompt(request.prompt),
        response: '',
        duration,
        tokensUsed: 0,
        cost: 0,
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error'
      };

      this.jobLogs.push(jobLog);
      this.activeJobs.delete(jobId);

      throw error;
    }
  }

  private async callModel(
    model: AIModel, 
    request: AIRequest, 
    signal: AbortSignal
  ): Promise<AIModelResponse> {
    // Simulate different model behaviors and response times
    const delay = model.latency === 'low' ? 500 : model.latency === 'medium' ? 1500 : 3000;
    
    await new Promise(resolve => setTimeout(resolve, delay));

    if (signal.aborted) {
      throw new Error('Request aborted');
    }

    // Mock response based on model capabilities
    const response = this.generateMockResponse(model, request.prompt);
    
    return {
      content: response,
      model: model.id,
      usage: {
        promptTokens: Math.floor(request.prompt.length / 4),
        completionTokens: Math.floor(response.length / 4),
        totalTokens: Math.floor((request.prompt.length + response.length) / 4)
      },
      metadata: {
        reasoning: this.generateReasoning(model, request.prompt),
        confidence: Math.random() * 0.3 + 0.7, // 70-100%
        suggestions: this.generateSuggestions(model, request.prompt)
      }
    };
  }

  private generateMockResponse(model: AIModel, prompt: string): string {
    const promptLower = prompt.toLowerCase();

    switch (model.id) {
      case 'gpt-4o':
        if (promptLower.includes('component')) {
          return `// Generated by ${model.name} - Core Developer & Architect
import React from 'react';

export function GeneratedComponent() {
  return (
    <div className="p-6 bg-white rounded-lg shadow-md">
      <h2 className="text-2xl font-bold mb-4">AI Generated Component</h2>
      <p className="text-gray-600">
        This component was generated based on your prompt: "${prompt}"
      </p>
      <button className="mt-4 px-4 py-2 bg-blue-500 text-white rounded hover:bg-blue-600">
        Get Started
      </button>
    </div>
  );
}`;
        }
        return `I'm GPT-4o, your Core Developer & Architect. I've analyzed your request: "${prompt}" and generated a comprehensive solution with proper architecture and component structure.`;

      case 'claude-3':
        return `As Claude 3, your Thought Partner & Safety Advisor, I've carefully considered your request: "${prompt}". 

From a business logic perspective, this approach makes sense because it aligns with user needs and follows best practices. I've also verified that the implementation is ethical and follows privacy guidelines.

Here are my recommendations for improving the user experience and ensuring the solution is both effective and responsible.`;

      case 'gemini-1.5':
        if (promptLower.includes('api') || promptLower.includes('database')) {
          return `// Generated by ${model.name} - Data Integration & Cloud Expert
// Optimized for Google Cloud Platform and Firebase

export async function handleApiIntegration() {
  try {
    const response = await fetch('/api/data', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ query: "${prompt}" })
    });
    
    const data = await response.json();
    return data;
  } catch (error) {
    console.error('API Integration Error:', error);
    throw error;
  }
}

// Firebase configuration optimized for your use case
export const firebaseConfig = {
  // Optimized settings based on your requirements
};`;
        }
        return `I'm Gemini 1.5, specializing in Data Integration & Cloud services. I've optimized your request for cloud deployment and API efficiency.`;

      case 'command-r-plus':
        return `üõ†Ô∏è Command R+ Analysis - Debugger & Optimizer

I've identified several optimization opportunities in your request: "${prompt}"

**Performance Improvements:**
- Reduced bundle size by 23%
- Optimized rendering with React.memo
- Implemented lazy loading for better UX

**Security Enhancements:**
- Added input validation
- Implemented CSRF protection
- Sanitized user inputs

**Code Quality:**
- Refactored for better maintainability
- Added TypeScript strict mode
- Improved error handling

The optimized solution is ready for production deployment.`;

      case 'mistral-8x':
        return `Quick response from Mistral 8x - Lightweight Assistant:

Here's a fast, efficient solution for: "${prompt}"

\`\`\`javascript
// Lightweight, optimized code
const solution = {
  fast: true,
  efficient: true,
  privacyFirst: true
};
\`\`\`

This solution prioritizes speed and privacy while maintaining functionality.`;

      case 'llava':
        return `ü§ñ LLaVA Visual Analysis:

I've analyzed the visual requirements for: "${prompt}"

**UI/UX Recommendations:**
- Modern, clean design with proper spacing
- Accessible color contrast ratios
- Mobile-first responsive layout
- Intuitive user flow

**Component Structure:**
- Header with navigation
- Main content area with cards
- Footer with links

The visual design follows current best practices and accessibility guidelines.`;

      default:
        return `Response generated for: "${prompt}"`;
    }
  }

  private generateReasoning(model: AIModel, prompt: string): string {
    return `${model.name} selected this approach because it aligns with the model's specialty in ${model.role.toLowerCase()} and optimizes for ${model.capabilities.slice(0, 2).join(' and ')}.`;
  }

  private generateSuggestions(model: AIModel, prompt: string): string[] {
    const baseSuggestions = [
      'Consider adding error handling',
      'Implement loading states',
      'Add accessibility features',
      'Optimize for mobile devices'
    ];

    const modelSpecificSuggestions: Record<string, string[]> = {
      'gpt-4o': ['Add TypeScript types', 'Implement unit tests', 'Consider code splitting'],
      'claude-3': ['Review business logic', 'Ensure ethical compliance', 'Improve user messaging'],
      'gemini-1.5': ['Add API caching', 'Implement database indexing', 'Consider cloud functions'],
      'command-r-plus': ['Profile performance', 'Add security headers', 'Implement monitoring'],
      'mistral-8x': ['Minimize bundle size', 'Add offline support', 'Optimize for speed'],
      'llava': ['Improve visual hierarchy', 'Add animations', 'Enhance color scheme']
    };

    return [...baseSuggestions, ...(modelSpecificSuggestions[model.id] || [])];
  }

  getJobLogs(projectId?: string): AIJobLog[] {
    return this.jobLogs.filter(log => !projectId || log.prompt.includes(projectId));
  }

  cancelJob(jobId: string): boolean {
    const controller = this.activeJobs.get(jobId);
    if (controller) {
      controller.abort();
      this.activeJobs.delete(jobId);
      return true;
    }
    return false;
  }

  getActiveJobs(): string[] {
    return Array.from(this.activeJobs.keys());
  }

  clearLogs(): void {
    this.jobLogs = [];
  }

  getUsageStats(userId?: string): {
    totalJobs: number;
    successRate: number;
    averageDuration: number;
    totalCost: number;
    modelUsage: Record<string, number>;
  } {
    const logs = this.jobLogs;
    const successfulJobs = logs.filter(log => log.success);
    
    const modelUsage = logs.reduce((acc, log) => {
      acc[log.modelUsed] = (acc[log.modelUsed] || 0) + 1;
      return acc;
    }, {} as Record<string, number>);

    return {
      totalJobs: logs.length,
      successRate: logs.length > 0 ? successfulJobs.length / logs.length : 0,
      averageDuration: logs.length > 0 ? logs.reduce((sum, log) => sum + log.duration, 0) / logs.length : 0,
      totalCost: logs.reduce((sum, log) => sum + log.cost, 0),
      modelUsage
    };
  }
}

// Global orchestrator instance
export const aiOrchestrator = new AIOrchestrator();